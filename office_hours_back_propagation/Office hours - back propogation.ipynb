{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Office Hours - Back Propagation\n",
    "\n",
    "## Intro\n",
    "\n",
    "[Transcript - Intro](#Transcript---Intro)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff we know\n",
    "\n",
    "[Transcript - Stuff we know](#Transcript---Stuff-we-know)\n",
    "\n",
    "Output layer math:\n",
    "\n",
    "1. Derivative of the logistic function \n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial \\hat{y}}{\\partial a_3}} = \\hat{y}(1 - \\hat{y})$$\n",
    "\n",
    "\n",
    "2. Derivative of the logistic cost function\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial \\hat{y}}} = \\hat{y} - y$$\n",
    "\n",
    "\n",
    "3. Chain rule of $g(x) = f(h(x))$:\n",
    "\n",
    "$$ \\boldsymbol{\\frac{\\partial g}{\\partial x}} = \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$$\n",
    "\n",
    "\n",
    "4. Weight update rule:\n",
    "\n",
    "$$\\boldsymbol{w} = w - \\alpha\\Delta w$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The example neural network for this exercise:\n",
    "\n",
    "[Transcript - Example neural network](#Transcript---Example-neural-network)\n",
    "\n",
    "* 2 input nodes\n",
    "* 2 hidden nodes\n",
    "* 1 output node\n",
    "\n",
    "![01-example-network.png](01-example-network.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At some point during training, the network looks like this:\n",
    "\n",
    "[Transcript - Example Training Weights](#Transcript---Example-Training-Weights)\n",
    "\n",
    "![02-example-weights.png](02-example-weights.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we encounter the example:\n",
    "\n",
    "$$[x_1, x_2, y] = [0, 1, 1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Forward Pass using the example:\n",
    "\n",
    "[Transcript - Forward Pass](#Transcript---Forward-Pass)\n",
    "\n",
    "$$\\boldsymbol{[x_1,x_2,y]} = [0,1,1]$$\n",
    "\n",
    "$$\\boldsymbol{a_1} =0(.1)+1(.3)=0+.3=.3$$\n",
    "\n",
    "$$\\boldsymbol{z_1} =log_{10}(.3) = .57$$\n",
    "\n",
    "$$\\boldsymbol{a_2}=0(.2)+1(.4)=0+.4=.4$$\n",
    "\n",
    "$$\\boldsymbol{z_2}=log_{10}(.4) = .60$$\n",
    "\n",
    "$$\\boldsymbol{a_3}=.57(.5)+.60(.6)=.646$$\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}}=log_{10}(.646) = .66$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = 0.30\n",
      "z1 = 0.57\n",
      "a2 = 0.40\n",
      "z2 = 0.60\n",
      "a3 = 0.65\n",
      "y_hat = 0.656206\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "import math\n",
    "\n",
    "w = [.1, .2, .3, .4, .5, .6]\n",
    "x1 = 0\n",
    "x2 = 1\n",
    "y = 1\n",
    "\n",
    "a1 = (x1 * w[0]) + (x2 * w[2])\n",
    "print(f'a1 = {a1:.2f}')\n",
    "\n",
    "z1 = expit(a1)\n",
    "print(f'z1 = {z1:.2f}')\n",
    "\n",
    "a2 = (x1 * w[1]) + (x2 * w[3])\n",
    "print(f'a2 = {a2:.2f}')\n",
    "\n",
    "z2 = expit(a2)\n",
    "print(f'z2 = {z2:.2f}')\n",
    "\n",
    "a3 = (z1 * w[4]) + (z2 * w[5])\n",
    "print(f'a3 = {a3:.2f}')\n",
    "\n",
    "y_hat = expit(a3)\n",
    "print(f'y_hat = {y_hat:.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "[Transcript - Calculate the Cost Function](#Transcript---Calculate-the-Cost-Function)\n",
    "\n",
    "\n",
    "cost $(y, \\hat{y}) = -log_{10}(\\hat{y})$ if $y = 1$\n",
    "\n",
    "$= .18$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.18 if y = 1\n"
     ]
    }
   ],
   "source": [
    "cost = -math.log10(y_hat)\n",
    "print(f'cost = {cost:.2f} if y = 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "\n",
    "[Transcript - Back Propagation - first layer - between the output later and the hidden layer](#Transcript---Back-Propagation---first-layer---between-the-output-later-and-the-hidden-layer)\n",
    "\n",
    "[Transcript - Back Propagation - next layer - between the hidden later and the first layer](#Transcript---Back-Propagation---next-layer---between-the-hidden-later-and-the-first-layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Back Propagation:\n",
    "\n",
    "* Create w_old\n",
    "* Alpha is the learning rate, which is a dampening function applied to the change specified by the error allocation.\n",
    "* $\\alpha = 0.2$, meaning that the error allocation will be reduced to 20% of the specified change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "w_old = w.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, update the hidden layer based on the output.  \n",
    "\n",
    "We use the chain rule twice to assign the errors to $w_5$ and $w_6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output layer math:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial w_5}}=\\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial w_5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of the logistic cost function\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial \\hat{y}}} = \\hat{y} - y = 0.66 - 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66 - 1.00 = -0.34\n"
     ]
    }
   ],
   "source": [
    "dE_dy_hat = y_hat - y\n",
    "\n",
    "print(f'{y_hat:.2f} - {y:.2f} = {dE_dy_hat:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of the logistic function \n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial \\hat{y}}{\\partial a_3}} = \\hat{y}(1 - \\hat{y}) = 0.66 (1 - 0.66)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66 (1 - 0.66) = 0.23\n"
     ]
    }
   ],
   "source": [
    "dy_hat_da_3 = y_hat * (1 - y_hat)\n",
    "\n",
    "print(f'{y_hat:.2f} (1 - {y_hat:.2f}) = {dy_hat_da_3:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial a_3}{\\partial w_5}} = z_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1 = 0.57\n"
     ]
    }
   ],
   "source": [
    "print(f'z1 = {z1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial a_3}{\\partial w_6}} = z_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z2 = 0.60\n"
     ]
    }
   ],
   "source": [
    "print(f'z2 = {z2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment for $w_5$:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial w_5}}=\\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial w_5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_dw_5 = -0.34 * 0.23 * 0.57 = -0.045\n"
     ]
    }
   ],
   "source": [
    "dE_dw_5 = dE_dy_hat * dy_hat_da_3 * z1\n",
    "\n",
    "print(f'dE_dw_5 = {dE_dy_hat:.2f} * {dy_hat_da_3:.2f} * {z1:.2f} = {dE_dw_5:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment for $w_6$:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial w_6}}=\\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial w_6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_dw_6 = -0.34 * 0.23 * 0.60 = -0.046\n"
     ]
    }
   ],
   "source": [
    "dE_dw_6 = dE_dy_hat * dy_hat_da_3 * z2\n",
    "\n",
    "print(f'dE_dw_6 = {dE_dy_hat:.2f} * {dy_hat_da_3:.2f} * {z2:.2f} = {dE_dw_6:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight update for $w_5$:\n",
    "\n",
    "$$\\boldsymbol{w_5} = w_5 - \\alpha\\Delta w_5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_5 = 0.50 - (0.20 * -0.045) = 0.51\n"
     ]
    }
   ],
   "source": [
    "w[4] = w_old[4] - (alpha * dE_dw_5)\n",
    "\n",
    "print(f'w_5 = {w_old[4]:.2f} - ({alpha:.2f} * {dE_dw_5:.3f}) = {w[4]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight update for $w_6$:\n",
    "\n",
    "$$\\boldsymbol{w_6} = w_6 - \\alpha\\Delta w_6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_6 = 0.60 - (0.20 * -0.046) = 0.61\n"
     ]
    }
   ],
   "source": [
    "w[5] = w_old[5] - (alpha * dE_dw_5)\n",
    "\n",
    "print(f'w_6 = {w_old[5]:.2f} - ({alpha:.2f} * {dE_dw_6:.3f}) = {w[5]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the next layer based on the previous updates.  \n",
    "\n",
    "We use the chain rule again assign the errors to $w_1$ through $w_4$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer math:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial w_1}} = \n",
    "\\frac{\\partial E}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial w_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial z_1}}\n",
    "= \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial z_1}\n",
    "= \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3} \\cdot w_5\n",
    "= (-0.34) \\cdot (0.23) \\cdot (0.5) \n",
    "= $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_dz_1 = -0.34 * 0.23 * 0.50 = -0.039\n"
     ]
    }
   ],
   "source": [
    "dE_dz_1 = dE_dy_hat * dy_hat_da_3 * w_old[4]\n",
    "\n",
    "print(f'dE_dz_1 = {dE_dy_hat:.2f} * {dy_hat_da_3:.2f} * {w_old[4]:.2f} = {dE_dz_1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial z_2}}\n",
    "= \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial z_2}\n",
    "= \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3} \\cdot w_6\n",
    "= (-0.34) \\cdot (0.23) \\cdot (0.6) \n",
    "= $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_dz_2 = -0.34 * 0.23 * 0.60 = -0.047\n"
     ]
    }
   ],
   "source": [
    "dE_dz_2 = dE_dy_hat * dy_hat_da_3 * w_old[5]\n",
    "\n",
    "print(f'dE_dz_2 = {dE_dy_hat:.2f} * {dy_hat_da_3:.2f} * {w_old[5]:.2f} = {dE_dz_2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial z_1}{\\partial a_1}} = a_1(1 - a_1) = 0.30 (1 - 0.30) = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_1_da_1 = 0.30 (1 - 0.30) = 0.21\n"
     ]
    }
   ],
   "source": [
    "dz_1_da_1 = a1 * (1 - a1)\n",
    "\n",
    "print(f'dz_1_da_1 = {a1:.2f} (1 - {a1:.2f}) = {dz_1_da_1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial z_2}{\\partial a_2}} = a_2(1 - a_2) = 0.40 (1 - 0.40) = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_2_da_2 = 0.40 (1 - 0.40) = 0.24\n"
     ]
    }
   ],
   "source": [
    "dz_2_da_2 = a2 * (1 - a2)\n",
    "\n",
    "print(f'dz_2_da_2 = {a2:.2f} (1 - {a2:.2f}) = {dz_2_da_2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial a_1}{\\partial w_1}} = x_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da_1_dw_1 = 0.00\n"
     ]
    }
   ],
   "source": [
    "da_1_dw_1 = x1\n",
    "\n",
    "print(f'da_1_dw_1 = {da_1_dw_1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial a_2}{\\partial w_2}} = x_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da_2_dw_2 = 0.00\n"
     ]
    }
   ],
   "source": [
    "da_2_dw_2 = x1\n",
    "\n",
    "print(f'da_2_dw_2 = {da_2_dw_2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial a_1}{\\partial w_3}} = x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da_1_dw_3 = 1.00\n"
     ]
    }
   ],
   "source": [
    "da_1_dw_3 = x2\n",
    "\n",
    "print(f'da_1_dw_3 = {da_1_dw_3:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\frac{\\partial a_2}{\\partial w_4}} = x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da_2_dw_4 = 1.00\n"
     ]
    }
   ],
   "source": [
    "da_2_dw_4 = x2\n",
    "\n",
    "print(f'da_2_dw_4 = {da_2_dw_4:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment for $w_1$:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial w_1}} = \n",
    "\\frac{\\partial E}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial w_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_dw_1 = -0.039 * 0.21 * 0.00 = -0.000\n"
     ]
    }
   ],
   "source": [
    "dE_dw_1 = dE_dz_1 * dz_1_da_1 * da_1_dw_1\n",
    "\n",
    "print(f'dE_dw_1 = {dE_dz_1:.3f} * {dz_1_da_1:.2f} * {da_1_dw_1:.2f} = {dE_dw_1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment for $w_2$:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial w_2}} = \n",
    "\\frac{\\partial E}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial w_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_dw_2 = -0.047 * 0.24 * 0.00 = -0.000\n"
     ]
    }
   ],
   "source": [
    "dE_dw_2 = dE_dz_2 * dz_2_da_2 * da_2_dw_2\n",
    "\n",
    "print(f'dE_dw_2 = {dE_dz_2:.3f} * {dz_2_da_2:.2f} * {da_2_dw_2:.2f} = {dE_dw_2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment for $w_3$:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial w_3}} = \n",
    "\\frac{\\partial E}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial w_3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_dw_3 = -0.039 * 0.21 * 1.00 = -0.008\n"
     ]
    }
   ],
   "source": [
    "dE_dw_3 = dE_dz_1 * dz_1_da_1 * da_1_dw_3\n",
    "\n",
    "print(f'dE_dw_3 = {dE_dz_1:.3f} * {dz_1_da_1:.2f} * {da_1_dw_3:.2f} = {dE_dw_3:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment for $w_4$:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial E}{\\partial w_4}} = \n",
    "\\frac{\\partial E}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial w_4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dE_dw_4 = -0.047 * 0.24 * 1.00 = -0.011\n"
     ]
    }
   ],
   "source": [
    "dE_dw_4 = dE_dz_2 * dz_2_da_2 * da_2_dw_4\n",
    "\n",
    "print(f'dE_dw_4 = {dE_dz_2:.3f} * {dz_2_da_2:.2f} * {da_2_dw_4:.2f} = {dE_dw_4:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight update for $w_1$:\n",
    "\n",
    "$$w_1 = w_1 - \\alpha\\Delta w_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_1 = 0.10 - (0.20 * -0.000) = 0.100\n"
     ]
    }
   ],
   "source": [
    "w[0] = w_old[0] - (alpha * dE_dw_1)\n",
    "\n",
    "print(f'w_1 = {w_old[0]:.2f} - ({alpha:.2f} * {dE_dw_1:.3f}) = {w[0]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight update for $w_2$:\n",
    "\n",
    "$$w_2 = w_2 - \\alpha\\Delta w_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_2 = 0.20 - (0.20 * -0.000) = 0.200\n"
     ]
    }
   ],
   "source": [
    "w[1] = w_old[1] - (alpha * dE_dw_2)\n",
    "\n",
    "print(f'w_2 = {w_old[1]:.2f} - ({alpha:.2f} * {dE_dw_2:.3f}) = {w[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight update for $w_3$:\n",
    "\n",
    "$$w_3 = w_3 - \\alpha\\Delta w_3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_3 = 0.30 - (0.20 * -0.008) = 0.302\n"
     ]
    }
   ],
   "source": [
    "w[2] = w_old[2] - (alpha * dE_dw_3)\n",
    "\n",
    "print(f'w_3 = {w_old[2]:.2f} - ({alpha:.2f} * {dE_dw_3:.3f}) = {w[2]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight update for $w_4$:\n",
    "\n",
    "$$w_4 = w_4 - \\alpha\\Delta w_4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_4 = 0.40 - (0.20 * -0.011) = 0.402\n"
     ]
    }
   ],
   "source": [
    "w[3] = w_old[3] - (alpha * dE_dw_4)\n",
    "\n",
    "print(f'w_4 = {w_old[3]:.2f} - ({alpha:.2f} * {dE_dw_4:.3f}) = {w[3]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run forward propagation again using updated weights to get a new prediction\n",
    "\n",
    "[Transcript - Run forward propagation again using updated weights to get a new prediction](#Transcript---Run-forward-propagation-again-using-updated-weights-to-get-a-new-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 = 0\n",
      "x2 = 1\n",
      "\n",
      "old\n",
      "w_old_1 = 0.1\n",
      "w_old_2 = 0.2\n",
      "w_old_3 = 0.3\n",
      "w_old_4 = 0.4\n",
      "w_old_5 = 0.5\n",
      "w_old_6 = 0.6\n",
      "a1 = 0.30\n",
      "z1 = 0.57\n",
      "a2 = 0.40\n",
      "z2 = 0.60\n",
      "a3 = 0.65\n",
      "y_hat_old = 0.656206\n",
      "\n",
      "new\n",
      "w_1 = 0.1\n",
      "w_2 = 0.2\n",
      "w_3 = 0.30162875345302287\n",
      "w_4 = 0.40223371902128857\n",
      "w_5 = 0.5089107165030491\n",
      "w_6 = 0.6089107165030491\n",
      "a1 = 0.30\n",
      "z1 = 0.57\n",
      "a2 = 0.40\n",
      "z2 = 0.60\n",
      "a3 = 0.66\n",
      "y_hat = 0.658680\n",
      "change = 0.002473435353621767\n"
     ]
    }
   ],
   "source": [
    "print(f'x1 = {x1}')\n",
    "print(f'x2 = {x2}')\n",
    "\n",
    "print('\\nold')\n",
    "[print(f'w_old_{i+1} = {w_old[i]}') for i in range(len(w))]\n",
    "print(f'a1 = {a1:.2f}')\n",
    "print(f'z1 = {z1:.2f}')\n",
    "print(f'a2 = {a2:.2f}')\n",
    "print(f'z2 = {z2:.2f}')\n",
    "print(f'a3 = {a3:.2f}')\n",
    "\n",
    "y_hat_old = y_hat.copy()\n",
    "print(f'y_hat_old = {y_hat_old:.6f}')\n",
    "\n",
    "\n",
    "print('\\nnew')\n",
    "\n",
    "\n",
    "[print(f'w_{i+1} = {w[i]}') for i in range(len(w))]\n",
    "\n",
    "\n",
    "a1 = (x1 * w[0]) + (x2 * w[2])\n",
    "print(f'a1 = {a1:.2f}')\n",
    "\n",
    "z1 = expit(a1)\n",
    "print(f'z1 = {z1:.2f}')\n",
    "\n",
    "a2 = (x1 * w[1]) + (x2 * w[3])\n",
    "print(f'a2 = {a2:.2f}')\n",
    "\n",
    "z2 = expit(a2)\n",
    "print(f'z2 = {z2:.2f}')\n",
    "\n",
    "a3 = (z1 * w[4]) + (z2 * w[5])\n",
    "print(f'a3 = {a3:.2f}')\n",
    "\n",
    "y_hat = expit(a3)\n",
    "print(f'y_hat = {y_hat:.6f}')\n",
    "\n",
    "print(f'change = {y_hat - y_hat_old}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Intro\n",
    "\n",
    "\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:00:29\n",
    "\n",
    "Good.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:00:46\n",
    "\n",
    "By the way, for preparation. This is super critical pen and paper. So if you don't have something to write with Andy good time to go get something\n",
    "\n",
    "00:02:23\n",
    "\n",
    "Get started.\n",
    "\n",
    "00:02:27\n",
    "\n",
    "Joining in a minute.\n",
    "\n",
    "00:02:29\n",
    "\n",
    "So here's what we're gonna do.\n",
    "\n",
    "00:02:36\n",
    "\n",
    "back propagation. So we want to go through the math on back propagation. We want to, you know, go through all the excruciating details.\n",
    "\n",
    "00:02:42\n",
    "\n",
    "So that'll be sort of done at once.\n",
    "\n",
    "00:02:45\n",
    "\n",
    "Hopefully that like\n",
    "\n",
    "00:02:48\n",
    "\n",
    "Helps internalize what it's doing is, is\n",
    "\n",
    "00:02:54\n",
    "\n",
    "In terms of how it's updating the parameters.\n",
    "\n",
    "00:02:58\n",
    "\n",
    "But we also don't want it to. It's all a ton of bookkeeping, as we'll see in a minute to do this. So we want to sort of do as little as possible.\n",
    "\n",
    "00:03:07\n",
    "\n",
    "In terms of like the minimum the minimum amount to see all of the action happening by propagation without doing any extra meaning any like repeat math things that we want to learn from. And so what we're going to do is take a very small neural network. This one.\n",
    "\n",
    "00:03:28\n",
    "\n",
    "Basically, it's like, what's the minimum we can do and still to learn everything we need no more revelation. And so that's this network that to input nodes to hidden notes and one open\n",
    "\n",
    "00:03:39\n",
    "\n",
    "So we're going to do one forward pass on this network and then going to one complete backward pass, and then we'll do one for plastic and see if you've improved them all.\n",
    "\n",
    "00:03:50\n",
    "\n",
    "I'll take a whole hour probably\n",
    "\n",
    "00:04:00\n",
    "\n",
    "So let's go work out my notes here.\n",
    "\n",
    "00:04:04\n",
    "\n",
    "A little bit.\n",
    "\n",
    "00:04:08\n",
    "\n",
    "So before we get into the network itself. There's a few things we want to just kind of list off. These are things that we've already encountered as a class in the sink.\n",
    "\n",
    "00:04:19\n",
    "\n",
    "So none of this is new. It's just things we know things we know that we're going to use here. So the first thing we know is that the derivative of the logistic function is why had times one minus y. Super simple. Right.\n",
    "\n",
    "00:04:36\n",
    "\n",
    "A few questions about any of this as we go along. Just, just chime in.\n",
    "\n",
    "00:04:41\n",
    "\n",
    "Second thing we know\n",
    "\n",
    "00:04:44\n",
    "\n",
    "Is that the derivative of the logistic function.\n",
    "\n",
    "00:04:48\n",
    "\n",
    "Will just sort of logistic costs function is simply why why hat minus why\n",
    "\n",
    "00:04:55\n",
    "\n",
    "It's really important. We get this sign right as it inside here. Otherwise, we'll get the wrong what we couldn't have update the parameters in the wrong direction to make the model.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:05:04\n",
    "\n",
    "Worse.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:05:08\n",
    "\n",
    "I'll go a little bit slow as well. So if you guys want to write this down along the way to come\n",
    "\n",
    "00:05:14\n",
    "\n",
    "To the third thing we know is that the chain rule states that if we have a function g of x.\n",
    "\n",
    "00:05:21\n",
    "\n",
    "That function can be\n",
    "\n",
    "00:05:36\n",
    "\n",
    "Chain rule states that function g of x can be described as a composition of to other functions of age of x.\n",
    "\n",
    "00:05:43\n",
    "\n",
    "And that can be applied.\n",
    "\n",
    "00:05:46\n",
    "\n",
    "To help us decompose the derivative into to constituent parts. So the first part being that the derivative of g with the changing g with respect to change in x.\n",
    "\n",
    "00:05:58\n",
    "\n",
    "Equals changing g with respect to the changing age, times the change in age with respect to the change in x. So care kind of see where this would come into play. Right, so like where we're at a particular note in the network and we have an activation function. The activation function is\n",
    "\n",
    "00:06:20\n",
    "\n",
    "Multiplied against the\n",
    "\n",
    "00:06:24\n",
    "\n",
    "The dot product. So essentially ideation, though, we have a composition of two functions. And so we can see how it might be valuable to be able to decompose\n",
    "\n",
    "00:06:35\n",
    "\n",
    "The derivative of those two functions.\n",
    "\n",
    "00:06:41\n",
    "\n",
    "Will come back 30 seconds and then the way up their role, which is simply the near way.\n",
    "\n",
    "00:06:49\n",
    "\n",
    "Equals the old ways, minus the learning rate alpha\n",
    "\n",
    "00:06:54\n",
    "\n",
    "Times the change in the way\n",
    "\n",
    "00:06:57\n",
    "\n",
    "delta w\n",
    "\n",
    "00:06:59\n",
    "\n",
    "weights of the parameters.\n",
    "\n",
    "00:07:04\n",
    "\n",
    "Questions about any of this stuff.\n",
    "\n",
    "00:07:11\n",
    "\n",
    "Will lead us off to the side, we can refer to it.\n",
    "\n",
    "00:07:22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Stuff we know\n",
    "\n",
    "\n",
    "\n",
    "Here's our, our network.\n",
    "\n",
    "00:07:26\n",
    "\n",
    "So labeled everything\n",
    "\n",
    "00:07:29\n",
    "\n",
    "So, or two features are x one x two.\n",
    "\n",
    "00:07:38\n",
    "\n",
    "At the hidden nodes. We're going to use to labels. We're going to have\n",
    "\n",
    "00:07:43\n",
    "\n",
    "A NZ. And so a one is the node it for the note value before the activation functions applied and Z one is node one, the hidden node one after that commission junction and apply\n",
    "\n",
    "00:08:00\n",
    "\n",
    "That's gonna be helpful to have that decomposition. Again, that's where the channels good play\n",
    "\n",
    "00:08:08\n",
    "\n",
    "And then\n",
    "\n",
    "00:08:09\n",
    "\n",
    "The output node. Again, the value before the activation functions can be called a three and the value after the activation function in this case where you call why happy because it's a prediction.\n",
    "\n",
    "00:08:21\n",
    "\n",
    "So we have it's fully connected. So we have a six. I just 6.6 parameters W one, W. W. W. W WC usually the literature you see these written with to some scripts, one for the\n",
    "\n",
    "00:08:39\n",
    "\n",
    "Essentially the layer. It's in and the other for the note within the layer. I'm just calling it was using one subscript here because this is super small network.\n",
    "\n",
    "00:08:49\n",
    "\n",
    "Because with me so far.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:08:51\n",
    "\n",
    "Todd. What would that look like if you had written it with the two sub scripts, can you\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:08:55\n",
    "\n",
    "Just say that this is to be like, there's no to be W one one and this, that would be W two to come on\n",
    "\n",
    "00:09:10\n",
    "\n",
    "Okay, so then we're going to say that at some point. I'm trying to process already ingrained to send this to Cathy greatness, then we're iterating through all the training examples.\n",
    "\n",
    "00:09:20\n",
    "\n",
    "That each point each training example we do a forward past, we compute the air then we do a backward pass. We update our model. And so some particular as we're both here is a big long trend is that for whatever problem we're trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Example neural network\n",
    "\n",
    "\n",
    "\n",
    "00:09:34\n",
    "\n",
    "At some particular moments. There's some particular example, we get to this point, which is that w one equals point one w two equals point to W three equals point three.\n",
    "\n",
    "00:09:45\n",
    "\n",
    "W four equals point forward, w equals point five and six equals point six obviously there arbitrary values, but you can imagine that at some point in training model looks like this. Right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Example Training Weights\n",
    "\n",
    "\n",
    "\n",
    "00:10:01\n",
    "\n",
    "So then the next example we encounter is this one x one equals zero, x two equals one and y equals one. So the expectation here is that inputs, zero and one, the predictions should be one. So, the air is going to be how far off from one it was.\n",
    "\n",
    "00:10:27\n",
    "\n",
    "Obviously I'm intentionally keeping this\n",
    "\n",
    "00:10:31\n",
    "\n",
    "Super simple, but also have everything we\n",
    "\n",
    "00:10:36\n",
    "\n",
    "Need to learn how this works.\n",
    "\n",
    "00:10:45\n",
    "\n",
    "Guys, good.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:10:48\n",
    "\n",
    "All right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Forward Pass\n",
    "\n",
    "\n",
    "\n",
    "Todd Holloway\n",
    "\n",
    "00:11:06\n",
    "\n",
    "So the first thing we want to do is at at that particular example for that particular example, we want to do a forecast, but I want to make a prediction.\n",
    "\n",
    "00:11:15\n",
    "\n",
    "So once you guys to work out the math and do a Ford British. Do you know how to do for prediction right done a bunch of times.\n",
    "\n",
    "00:11:23\n",
    "\n",
    "Already so much to do with this network right here in the middle of the right hand screen are still for past with this example.\n",
    "\n",
    "00:11:32\n",
    "\n",
    "So, you know, you're gonna you're gonna take zero here as input for x one and you'd say one. And so this node before activation is going to be zero times point one plus one times point three.\n",
    "\n",
    "00:11:47\n",
    "\n",
    "And then you're gonna put that through the logistic function, you can use Wolfram Alpha to help you with that one.\n",
    "\n",
    "00:11:54\n",
    "\n",
    "Is everyone familiar with both Wolfram Alpha\n",
    "\n",
    "00:12:03\n",
    "\n",
    "It's a more than that. But it's like have an island graphing calculator puzzle a knowledge base.\n",
    "\n",
    "00:12:26\n",
    "\n",
    "So you can just type logistic function right into the info box.\n",
    "\n",
    "00:12:34\n",
    "\n",
    "Just like logistic whatever you want.\n",
    "\n",
    "00:12:39\n",
    "\n",
    "So you do that, you get the logistic function value of point three.\n",
    "\n",
    "00:12:44\n",
    "\n",
    "And then\n",
    "\n",
    "00:12:48\n",
    "\n",
    "A three is going to be W five plus sorry it's gonna be 01 times w five plus the two times w six times. And we're going to use logistic function again. So, again, to keep things simple. We just need to push for all activation just totally fine.\n",
    "\n",
    "00:13:12\n",
    "\n",
    "So if you plug that in.\n",
    "\n",
    "00:13:14\n",
    "\n",
    "And you want to keep your bookkeeping around so you want to again meticulously lay this out on paper. So everyone is going to equal zero plus point three equals point 3010 we should get point five, seven.\n",
    "\n",
    "00:13:31\n",
    "\n",
    "Let me know if you have trouble getting that from open alpha\n",
    "\n",
    "00:13:35\n",
    "\n",
    "A two is going to be zero plus point four because point for z two is going to be again logistic point four, which is point six\n",
    "\n",
    "00:13:47\n",
    "\n",
    "And a three is going to be\n",
    "\n",
    "00:13:50\n",
    "\n",
    "Point five, seven times point five plus point six times point six\n",
    "\n",
    "00:13:58\n",
    "\n",
    "Gives you point five to five and then we have our prediction about predictions going to be the logistic appoint by two.\n",
    "\n",
    "00:14:13\n",
    "\n",
    "So I'm going to pause for a minute. I'm going to pause actually even possible five minutes, but you guys work through this board pass, make sure everyone gets the solution. If you have questions or comments just shut them out.\n",
    "\n",
    " Amy Breden\n",
    "\n",
    "00:14:31\n",
    "\n",
    "link to that calculator in the chat.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:14:38\n",
    "\n",
    "Box.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:14:43\n",
    "\n",
    "I don't see it in the chat box.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:14:45\n",
    "\n",
    "Oh,\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:14:51\n",
    "\n",
    "Sorry.\n",
    "\n",
    "00:16:39\n",
    "\n",
    "Oh, you just like to segue into Wolfram Alpha\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:16:43\n",
    "\n",
    "5.25\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:17:07\n",
    "\n",
    "Same result.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:17:15\n",
    "\n",
    "Todd, I am sort of a side question i for some reason I came away from the ACR thinking that the sigmoid was the same as the hyperbolic tangent function. It's not\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:17:27\n",
    "\n",
    "It's not technically\n",
    "\n",
    "00:17:31\n",
    "\n",
    "Technically sigmoid is a classic functions. It's any function that has that shaped\n",
    "\n",
    "00:17:36\n",
    "\n",
    "One instance of a stigma that is attached image.\n",
    "\n",
    "00:17:40\n",
    "\n",
    "But people often and practices sigma logistic function or terribly\n",
    "\n",
    "00:17:45\n",
    "\n",
    "Regardless of 10 inches between is it as a sigmoid function between minus one and one and the logistic function as a as a sigmoid between zero and one.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:17:56\n",
    "\n",
    "And when we type in sigmoid into Wolfram Alpha, which gives us religious just take\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:18:09\n",
    "\n",
    "The 10 ish is is the lot of literature, because\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:18:14\n",
    "\n",
    "People sometimes find a convergence pastor, the logistic problem because of the negative numbers.\n",
    "\n",
    " Rajiv Nair\n",
    "\n",
    "00:18:38\n",
    "\n",
    "What is it actually doing the signal. I mean like when you. What does it mean when you put in a number and it gives you a percent. Is that a percentage of, like, what is it actually\n",
    "\n",
    "00:18:50\n",
    "\n",
    "In terms of the input. What is it, actually, what does it mean\n",
    "\n",
    "00:18:54\n",
    "\n",
    "Like a sigma 2.3 is equal 2.57 but what does that I understand the formula, but I don't quite understand what is it telling me about\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:19:06\n",
    "\n",
    "What is it like, what's the point\n",
    "\n",
    " Rajiv Nair\n",
    "\n",
    "00:19:09\n",
    "\n",
    "Yeah.\n",
    "\n",
    "00:19:11\n",
    "\n",
    "Yeah.\n",
    "\n",
    "00:19:26\n",
    "\n",
    "slightly off topic.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:19:32\n",
    "\n",
    "So it's just the properties that were concerned with. Right. So the properties are that it's ask them one and zero. So we're saying we're saying we want\n",
    "\n",
    "00:19:47\n",
    "\n",
    "So we're saying we want a output that's always going to be bounded between zero and one.\n",
    "\n",
    "00:19:54\n",
    "\n",
    "It's continuous so that it's differentiable at all points.\n",
    "\n",
    "00:20:06\n",
    "\n",
    "And that's the main thing is basically just the shape of it intuitively for trying to do with activation function is is\n",
    "\n",
    "00:20:14\n",
    "\n",
    "Squash a value in a certain range at least once it was intended, with the SIG one\n",
    "\n",
    "00:20:20\n",
    "\n",
    "Itself was the squash evaluate a certain range.\n",
    "\n",
    "00:20:24\n",
    "\n",
    "In a way that's continuously differentiable. And so like it's those two those two aspects of it that are make it appealing, but again, we could use other functions that looks similar.\n",
    "\n",
    "00:20:36\n",
    "\n",
    "Can he can have the same shape but\n",
    "\n",
    "00:20:40\n",
    "\n",
    "With different Awesome job.\n",
    "\n",
    "00:20:49\n",
    "\n",
    "There's like 10 inches gonna have the same properties because I have some type of value in this case minus one on one. And it's continuously differentiable\n",
    "\n",
    "00:20:59\n",
    "\n",
    "And so like\n",
    "\n",
    "00:21:02\n",
    "\n",
    "And then you can experiment with that you could change. You could use a different function as a different slope. And that's going to mean different things for the activation of the notes.\n",
    "\n",
    "00:21:12\n",
    "\n",
    "And then again in recent years, people have challenged this by using functions like Ray lose rectified activations that aren't us and tonic at all. And so, and that has different set of properties.\n",
    "\n",
    " Rajiv Nair\n",
    "\n",
    "00:21:25\n",
    "\n",
    "So does it activated. If it's over 50% or something like that.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:21:29\n",
    "\n",
    "Oh yeah, what does it mean in terms of a threshold. So that's a separate thing is like you can apply a threshold to it.\n",
    "\n",
    "00:21:36\n",
    "\n",
    "In the same way, to apply. So you could treat this like a backup so you can treat this as a probability right you know\n",
    "\n",
    "00:21:43\n",
    "\n",
    "That's in particular. That's why, that's why value between zero and one was popular for a long time because it's interpreted as a probability\n",
    "\n",
    "00:21:51\n",
    "\n",
    "And in the same way that you know when we talk about like naive Bayes which was probability or decision tree, which was possibility.\n",
    "\n",
    "00:21:57\n",
    "\n",
    "We could apply a threshold to convert that to predict the label. And so, so if we want we can use a threshold here to to convert if it's two classes to convert into a predictions and if it's more than two classes, we can use a soft max to convert into prediction.\n",
    "\n",
    "00:22:14\n",
    "\n",
    "And soft max is just takes the highest probability\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:22:19\n",
    "\n",
    "Basically\n",
    "\n",
    " Kevin Stone\n",
    "\n",
    "00:22:22\n",
    "\n",
    "Taught yeah I'm probably gonna embarrass myself here.\n",
    "\n",
    "00:22:28\n",
    "\n",
    "So that the calculate like a three. It's just Z one W five\n",
    "\n",
    "00:22:35\n",
    "\n",
    "Plus z three w six correct\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:22:41\n",
    "\n",
    "Yes, so the 01\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:22:46\n",
    "\n",
    "Plus point six\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:22:49\n",
    "\n",
    "2.57 times point five plus points six times point six and that will give you a three. I got\n",
    "\n",
    "00:23:00\n",
    "\n",
    "Point 6456 double check them out. So\n",
    "\n",
    " Bruno Todescan\n",
    "\n",
    "00:23:06\n",
    "\n",
    "So tonight.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:23:40\n",
    "\n",
    "I'm wrong.\n",
    "\n",
    " Kevin Stone\n",
    "\n",
    "00:23:43\n",
    "\n",
    "Yeah.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:24:06\n",
    "\n",
    "Alright, so it should be point six five.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:24:11\n",
    "\n",
    "Good catch.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:24:35\n",
    "\n",
    "Okay. Does everyone want to have that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Calculate the Cost Function\n",
    "\n",
    "\n",
    "\n",
    "00:24:45\n",
    "\n",
    "And then measure the error of the next step right\n",
    "\n",
    "00:24:49\n",
    "\n",
    "So how close. Obviously, if we're doing absolutely there it's point three five off.\n",
    "\n",
    "00:25:07\n",
    "\n",
    "We're not doing absolutely right, we're doing. We're using the surrogate cost function of the logistic function.\n",
    "\n",
    "00:25:13\n",
    "\n",
    "was covered in the basic\n",
    "\n",
    "00:25:17\n",
    "\n",
    "Which comes out to be minus log of why had if y is intended to be one.\n",
    "\n",
    "00:25:23\n",
    "\n",
    "So make sure enough, my math and still off.\n",
    "\n",
    " Bruno Todescan\n",
    "\n",
    "00:25:29\n",
    "\n",
    "Content just to confirm, that's a natural longer than right yeah\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:26:45\n",
    "\n",
    "Yeah, you should get minus\n",
    "\n",
    "00:26:52\n",
    "\n",
    "Sorry positive\n",
    "\n",
    "00:26:59\n",
    "\n",
    "One nine now.\n",
    "\n",
    " Amy Breden\n",
    "\n",
    "00:27:12\n",
    "\n",
    "Are we doing. Are we just taking the natural log of point six 5am I getting that right\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:27:29\n",
    "\n",
    "Point four three.\n",
    "\n",
    " Shaji Kunjumohamed\n",
    "\n",
    "00:27:33\n",
    "\n",
    "I might have to use the calculator wrong. Yeah.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:27:39\n",
    "\n",
    "So as long as to one basis.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:27:42\n",
    "\n",
    "It's lobbies 10 yeah\n",
    "\n",
    " Kevin Stone\n",
    "\n",
    "00:27:59\n",
    "\n",
    "So it's not the natural log\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:28:02\n",
    "\n",
    "Sorry log base time\n",
    "\n",
    "00:28:11\n",
    "\n",
    "So Wolfram Alpha is to minus log\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:28:16\n",
    "\n",
    "Minus long time\n",
    "\n",
    "00:28:33\n",
    "\n",
    "Okay.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:28:34\n",
    "\n",
    "So that's going to be a logistic error. So it's in a sense of depth in the air a little bit by using the logistic loss function.\n",
    "\n",
    " Rajiv Nair\n",
    "\n",
    "00:28:47\n",
    "\n",
    "I'm sorry. How did you get that formula for cost.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:28:53\n",
    "\n",
    "So if you're calling a sink introduced want to talk about logistic function.\n",
    "\n",
    "00:28:59\n",
    "\n",
    "We introduced the surrogate cost functions letter B convex\n",
    "\n",
    "00:29:05\n",
    "\n",
    "So I'm just using that cost question. So with any of these things that you can show you the record place other things, right. So we didn't have to use this activation function of being unhappy users cost function, but these are the ones up choosing us\n",
    "\n",
    "00:29:18\n",
    "\n",
    "And why am I choosing to use well. Which isn't to use\n",
    "\n",
    "00:29:22\n",
    "\n",
    "The logistic function for\n",
    "\n",
    "00:29:25\n",
    "\n",
    "The activation functions because it's\n",
    "\n",
    "00:29:28\n",
    "\n",
    "Sort of the most straightforward to do here.\n",
    "\n",
    "00:29:32\n",
    "\n",
    "And the one we talked with most of class.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:29:41\n",
    "\n",
    "CAN YOU EXPLAIN I'M. Where does a three come from. It doesn't mean that number still valid point five to five.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:29:50\n",
    "\n",
    "So,\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:29:53\n",
    "\n",
    "That's this number. So that's the value of the last note the\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:29:58\n",
    "\n",
    "Output\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:30:00\n",
    "\n",
    "For\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:30:06\n",
    "\n",
    "Alright. So yeah, if you multiply three with a one and a two. Yeah, that's what you would get. Okay.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:30:51\n",
    "\n",
    "Okay.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Back Propagation - first layer - between the output later and the hidden layer\n",
    "\n",
    "\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:30:54\n",
    "\n",
    "We'll move on to the back propagation\n",
    "\n",
    "00:31:20\n",
    "\n",
    "Okay, so back obligations so\n",
    "\n",
    "00:31:24\n",
    "\n",
    "Where do you layer by layer backwards. So the first layer. We want to update is the between the output later and the hidden layer one of those parameters. Right. And so the way to think about\n",
    "\n",
    "00:31:37\n",
    "\n",
    "back propagation\n",
    "\n",
    "00:31:48\n",
    "\n",
    "When we have more nodes on the on the previous layer. In this case, since we have two. Is that what we want to do is, is take our error and assign a like\n",
    "\n",
    "00:32:02\n",
    "\n",
    "Sign a fraction to it that we believe that the parameter on one of those notes is responsible for right so basically want to say how much of that area, we believe, W. Five is responsible for and how much of the air w six responsible for\n",
    "\n",
    "00:32:16\n",
    "\n",
    "Does that make sense.\n",
    "\n",
    "00:32:19\n",
    "\n",
    "This is where so that's where we're going to use the chain rule.\n",
    "\n",
    "00:32:23\n",
    "\n",
    "To do a partial derivatives.\n",
    "\n",
    "00:32:27\n",
    "\n",
    "To that assignment so\n",
    "\n",
    "00:32:30\n",
    "\n",
    "We're going to get there by taking the partial derivative twice that sort of taking the chain rule twice. Right. So we're going to say\n",
    "\n",
    "00:32:36\n",
    "\n",
    "We have the era. Now, right, which was the point\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:32:42\n",
    "\n",
    "So that was\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:32:44\n",
    "\n",
    "Where was was one night, right, we said\n",
    "\n",
    "00:32:50\n",
    "\n",
    "We have the arrow. So we're gonna say the derivative of the air with respect to w five over the air with just like a W five equals the derivative, the air change in the air with respect the change and white hat.\n",
    "\n",
    "00:33:05\n",
    "\n",
    "Times the change in y hat with respect to the change in a three.\n",
    "\n",
    "00:33:10\n",
    "\n",
    "Times the change in a three with respect to change and Wi Fi.\n",
    "\n",
    "00:33:18\n",
    "\n",
    "So what we're doing here.\n",
    "\n",
    "00:33:23\n",
    "\n",
    "Is we're looking at\n",
    "\n",
    "00:33:27\n",
    "\n",
    "The derivative of the slow with respect to check the change in in each between each of these, these items right the change in a three.\n",
    "\n",
    "00:33:36\n",
    "\n",
    "With respect to w five. So it's really the change with respect to z one\n",
    "\n",
    "00:33:41\n",
    "\n",
    "Change from a free from sorry from 012 a three given a change in w\n",
    "\n",
    "00:33:47\n",
    "\n",
    "change in y hat.\n",
    "\n",
    "00:33:50\n",
    "\n",
    "From\n",
    "\n",
    "00:33:52\n",
    "\n",
    "Z one times w given a three.\n",
    "\n",
    "00:34:00\n",
    "\n",
    "That's going to help us a sign that that attribution that partial iteration.\n",
    "\n",
    "00:34:05\n",
    "\n",
    "So if we break these out. So how do we do that when we first we did two channels. Right. So the first gen gave us web site change changing the air with respect to Wi Fi.\n",
    "\n",
    "00:34:21\n",
    "\n",
    "decomposes. The change in the air with respect to y hat times the change in white hat with respect to change in Wi Fi and then we've we've had a terrible again broken out once more.\n",
    "\n",
    "00:34:33\n",
    "\n",
    "And so what are these values. Well, the first one that changing the air with respect to change and white hat.\n",
    "\n",
    "00:34:42\n",
    "\n",
    "Is going to be the derivative of the cost function, right, because what is the relationship between my hat and the air. Well, the cost function, what's the derivative, the cost of actually going to change. Right. And so it's by definition.\n",
    "\n",
    "00:34:57\n",
    "\n",
    "And then the second item is was the change in y hat with respect to the change. So with respect to change in a three. And that's the job of the logistic function.\n",
    "\n",
    "00:35:10\n",
    "\n",
    "By definition,\n",
    "\n",
    "00:35:11\n",
    "\n",
    "Right.\n",
    "\n",
    "00:35:13\n",
    "\n",
    "And then how does a three change with respect to a change in Wi Fi or changes as you want, right, because a three equals, you want to be fine.\n",
    "\n",
    "00:35:27\n",
    "\n",
    "Super simple. Right.\n",
    "\n",
    "00:35:35\n",
    "\n",
    "Questions on that.\n",
    "\n",
    "00:35:57\n",
    "\n",
    "It's just bookkeeping. It's all like if you break down each constituent part super, super simple.\n",
    "\n",
    "00:36:04\n",
    "\n",
    "It's just all the bookkeeping is Mr.\n",
    "\n",
    " Bruno Todescan\n",
    "\n",
    "00:36:09\n",
    "\n",
    "Todd, are these are these partial derivatives.\n",
    "\n",
    "00:36:15\n",
    "\n",
    "Yes, he may be partial derivatives. Yeah.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:36:18\n",
    "\n",
    "It's partially because it's the change in error with respect to just, okay, great.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:36:23\n",
    "\n",
    "Okay, great.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:36:35\n",
    "\n",
    "So go ahead and run those numbers you can use your values, instead of my philosophy is point six times 2.6 you should come on guys are pretty similar line.\n",
    "\n",
    " Shaji Kunjumohamed\n",
    "\n",
    "00:36:51\n",
    "\n",
    "So the third one question for number one you have. Why had into one one minus Y hat. Right. It has to be sigmoid into one minus the sigmoid. We had tried\n",
    "\n",
    "00:37:09\n",
    "\n",
    "Seek more way had into one minus seek mode, we had right or\n",
    "\n",
    "00:37:16\n",
    "\n",
    "Or where is this. Oh, the, the stuff we know the number one debate you have the cost logistic function.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:37:26\n",
    "\n",
    "Or what about it. Yeah.\n",
    "\n",
    " Shaji Kunjumohamed\n",
    "\n",
    "00:37:28\n",
    "\n",
    "It is why you had to do one minus Y hat. Right, so it has to be sigmoid of white hat into one minus sigmoid of white hat. Right. It has to be like that right or am I\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:37:41\n",
    "\n",
    "Know, it was so delicious a function that derivative of it is why\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:37:47\n",
    "\n",
    "Okay, yes, this is the definition I wrote down here.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:37:55\n",
    "\n",
    "Again, the order is really important. If you mix up the order and all this bookkeeping, you make some design, you'll end up probably changing the grammar in the wrong direction.\n",
    "\n",
    "00:38:16\n",
    "\n",
    "Okay, so again we're gonna take a few minutes here and if everyone can try to write this down.\n",
    "\n",
    "00:38:23\n",
    "\n",
    "Again, so\n",
    "\n",
    "00:38:25\n",
    "\n",
    "Multiple so you can use point six five to prefer a point six five, you should at the end she represented results and okay\n",
    "\n",
    "00:38:33\n",
    "\n",
    "Point six five minus one would be\n",
    "\n",
    "00:38:38\n",
    "\n",
    "The first component\n",
    "\n",
    "00:38:42\n",
    "\n",
    "Because that's this and then point six five times one minus point six five.\n",
    "\n",
    "00:38:48\n",
    "\n",
    "And then times\n",
    "\n",
    "00:38:51\n",
    "\n",
    "01 which was points, five, seven,\n",
    "\n",
    "00:38:56\n",
    "\n",
    "And then she come out to value around might be a little different around minus point 00 0.05\n",
    "\n",
    "00:39:06\n",
    "\n",
    "And I do the same thing, but do it for w six\n",
    "\n",
    "00:39:26\n",
    "\n",
    "And I use a learning right here points here, which is really high.\n",
    "\n",
    "00:39:30\n",
    "\n",
    "What it illustrated, which is the point.\n",
    "\n",
    "00:39:35\n",
    "\n",
    "And even with a really high learning rate, you'll notice that our new ways is only going to be 100 larger right so we go from point five 2.51\n",
    "\n",
    "00:39:49\n",
    "\n",
    "And from point six one.\n",
    "\n",
    "00:39:53\n",
    "\n",
    "Are so it back propagation is timeless to this point is that it believes for w and w six or too small.\n",
    "\n",
    "00:40:02\n",
    "\n",
    "At least with respect to that particular trend example.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:40:20\n",
    "\n",
    "In practice, how can we be positive numbers for the changes because we just have to negative values in the parentheses.\n",
    "\n",
    "00:40:32\n",
    "\n",
    "Now we don't. Okay. Sorry, I just got it everyone\n",
    "\n",
    " Amy Breden\n",
    "\n",
    "00:40:38\n",
    "\n",
    "For the learning rate. How do you in practice actually figure out what that should be like you just pick point two, but I might have picked one. I don't like I don't know what the magnitude should be\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:40:52\n",
    "\n",
    "In practice, these days, people most often adaptive algorithms.\n",
    "\n",
    "00:40:57\n",
    "\n",
    "Like RMS prop. And so these are algorithms. They're built in the cycle learning TensorFlow and carries\n",
    "\n",
    "00:41:05\n",
    "\n",
    "Their algorithms that will like the initialized to value, but then they change it. So the idea is that like as you start converting\n",
    "\n",
    "00:41:13\n",
    "\n",
    "You want smaller and smaller like basically early, early in process to really highlight any right so you move quickly to the better values. But then as you start to converge. You want small it's parlor anyways.\n",
    "\n",
    " Amy Breden\n",
    "\n",
    "00:41:26\n",
    "\n",
    "And it just happens to be very similar to what our error was that's not at all related\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:41:34\n",
    "\n",
    "What is the weather.\n",
    "\n",
    " Amy Breden\n",
    "\n",
    "00:41:36\n",
    "\n",
    "Or error was point one nine but the learning that you chose was point two. That's not related. Right. Okay.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:41:55\n",
    "\n",
    "You guys all good roughly the same result.\n",
    "\n",
    "00:42:11\n",
    "\n",
    "Slide this over here as you can still see you want to bring up\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Back Propagation - next layer - between the hidden later and the first layer\n",
    "\n",
    "\n",
    "\n",
    "00:42:17\n",
    "\n",
    "This is where it gets to be a lot of bookkeeping.\n",
    "\n",
    "00:42:20\n",
    "\n",
    "The hidden layer.\n",
    "\n",
    "00:42:24\n",
    "\n",
    "Same process. But now, obviously we have, first of all, we have four inches before parameters.\n",
    "\n",
    "00:42:33\n",
    "\n",
    "And second of all, to compute an update to W one, we have to account for the update to Wi Fi. So they start to compose your parlance of every layer you add in that work as a backdrop. It's more and more bookkeeping.\n",
    "\n",
    "00:42:49\n",
    "\n",
    "And so same formula. It's still\n",
    "\n",
    "00:42:54\n",
    "\n",
    "Change the air with respect, you're changing W one decompose to the three constituent parts.\n",
    "\n",
    "00:43:01\n",
    "\n",
    "And the second two parts are still pretty straightforward. So the change in a one with respect to change in w one is going to be x one, which in this case x one is zero right zero\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:43:17\n",
    "\n",
    "So,\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:43:23\n",
    "\n",
    "X one.\n",
    "\n",
    "00:43:26\n",
    "\n",
    "So that's the same as before, right. So it's the same same Z one here, we have x one here. So, then a changing XE one with respect to change in the area one. Well, that's the activation function.\n",
    "\n",
    "00:43:40\n",
    "\n",
    "Derivative so\n",
    "\n",
    "00:43:43\n",
    "\n",
    "And we're using the same activation function. That means the output layer. So it's going to be here. It's going to be point three times one minus\n",
    "\n",
    "00:43:51\n",
    "\n",
    "Y hat effectively the prediction on the hidden node why at times one minus point\n",
    "\n",
    "00:44:00\n",
    "\n",
    "You kind of think of it as like, part of the reason back propagation is a kind of why was back probably isn't hard to embed like why did it take people you know 10 plus years to a method\n",
    "\n",
    "00:44:11\n",
    "\n",
    "Like part of its figuring out like you're trying to assign error on the hidden knows, but you don't know what it is they're supposed to be predicting right\n",
    "\n",
    "00:44:21\n",
    "\n",
    "So back propagation in a way of saying is we're making a claim about what the hidden known should be predicting and what the error of the hidden note is\n",
    "\n",
    "00:44:33\n",
    "\n",
    "OK, so it's these two components. Pretty straightforward. They look a lot like on the output layer, the least two components.\n",
    "\n",
    "00:44:40\n",
    "\n",
    "But the same, right. So the difference is that when it gets to this component you try to compete this now. It's a composition of all the previous players.\n",
    "\n",
    "00:44:49\n",
    "\n",
    "And so for me, a simple Valley having one edge there and one up a note. But even so, we've now got that change in the air with respect to changes in\n",
    "\n",
    "00:45:02\n",
    "\n",
    "Z one\n",
    "\n",
    "00:45:06\n",
    "\n",
    "Is defined as the change in the air with respect to change and what has changed. What have a three change and change in a three, which is like the shamans he wants. So now you've put everything you computed over here into this term.\n",
    "\n",
    "00:45:22\n",
    "\n",
    "That we added a second hidden layer. We will now have\n",
    "\n",
    "00:45:27\n",
    "\n",
    "Everything every component it computed here as the single term in that\n",
    "\n",
    "00:45:33\n",
    "\n",
    "Competition.\n",
    "\n",
    "00:45:36\n",
    "\n",
    "Make sense\n",
    "\n",
    "00:45:42\n",
    "\n",
    "To try to read this out of become\n",
    "\n",
    "00:46:08\n",
    "\n",
    "Try to write this out for all four winds, if you do that.\n",
    "\n",
    "00:46:13\n",
    "\n",
    "We can update the model and then we can do another forward pass and see if recalls better\n",
    "\n",
    "00:46:55\n",
    "\n",
    "This all makes sense or isn't clear as one\n",
    "\n",
    "00:47:11\n",
    "\n",
    "Kevin, what do you think\n",
    "\n",
    " Kevin Stone\n",
    "\n",
    "00:47:17\n",
    "\n",
    "Yeah, it makes sense. I'm\n",
    "\n",
    "00:47:20\n",
    "\n",
    "Looking at it but\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:47:23\n",
    "\n",
    "Thanks.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:47:34\n",
    "\n",
    "Okay, so we'll wait for the first person. The first person who comes up with all of the updates, let us know.\n",
    "\n",
    "00:48:21\n",
    "\n",
    "It's not that hard, right, once you go through it all, once you kind of like, you know, not that complicated algorithm is just a lot of bookkeeping. Right.\n",
    "\n",
    "00:48:33\n",
    "\n",
    "It's just a lot of bookkeeping and you just need to know. You need to have the continuous derivatives continuously differentiable function.\n",
    "\n",
    "00:48:56\n",
    "\n",
    "You can see how there's the problem of vanishing gradient, as it's called, where as you add more layers. The numbers you're multiplying keep getting smaller and smaller. And so the left most layers start to become\n",
    "\n",
    "00:49:15\n",
    "\n",
    "Not easy not easily updatable not easily updated.\n",
    "\n",
    "00:49:20\n",
    "\n",
    "Which is why people have moved away from\n",
    "\n",
    "00:49:23\n",
    "\n",
    "These activation functions that have a\n",
    "\n",
    "00:49:27\n",
    "\n",
    "Less shape to them.\n",
    "\n",
    "00:49:30\n",
    "\n",
    "Toward ones that have\n",
    "\n",
    "00:49:33\n",
    "\n",
    "Like a rally.\n",
    "\n",
    "00:49:35\n",
    "\n",
    "That is not a synthetic\n",
    "\n",
    "00:49:38\n",
    "\n",
    "That does not just approach a number\n",
    "\n",
    "00:49:42\n",
    "\n",
    "Where there's a hard part\n",
    "\n",
    "00:49:47\n",
    "\n",
    "Because the vanishing gradient, the small numbers, but come from.\n",
    "\n",
    "00:49:51\n",
    "\n",
    "If you're applying activation, the smaller and smaller numbers.\n",
    "\n",
    "00:49:55\n",
    "\n",
    "Sort of numbers are closer, closer to 01 or minus one on one. It's smaller and smaller numbers back in sponsored updates.\n",
    "\n",
    "00:50:02\n",
    "\n",
    "Which means the miles that learning quickly,\n",
    "\n",
    " Padma Sridhar\n",
    "\n",
    "00:50:19\n",
    "\n",
    "In the derivative of the errors with respect to w one is that coming from point three times one minus\n",
    "\n",
    "00:50:29\n",
    "\n",
    "Point five seven\n",
    "\n",
    "00:50:34\n",
    "\n",
    "You're saying this number right here 23 or four this point you want. Yeah.\n",
    "\n",
    "00:50:40\n",
    "\n",
    "This point to one. Yeah.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "00:50:43\n",
    "\n",
    "Is\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:50:46\n",
    "\n",
    "Yes, it was a point three times one minus point three of using slightly different numbers. I mean, you might get something slightly different.\n",
    "\n",
    "00:51:01\n",
    "\n",
    "Dogs are in the background.\n",
    "\n",
    "00:51:04\n",
    "\n",
    "My lectures.\n",
    "\n",
    "00:51:21\n",
    "\n",
    "Questions.\n",
    "\n",
    "00:51:25\n",
    "\n",
    "All the way it's something\n",
    "\n",
    "00:51:56\n",
    "\n",
    "Again she got a slightly closer prediction one after you've updated them all at one time.\n",
    "\n",
    "00:52:03\n",
    "\n",
    "And you might want to like if you'd be worried if it was too big of a jump in that direction. I'd be a red flag.\n",
    "\n",
    "00:52:11\n",
    "\n",
    "We definitely but but obviously if you\n",
    "\n",
    "00:52:14\n",
    "\n",
    "Listen to think about like to take it to an extreme. If you just have one training example and he kept doing exactly what we did. Over and over again, it should converge on a perfect prediction in short order order right to be overfitting, so to speak, or maybe\n",
    "\n",
    "00:52:28\n",
    "\n",
    "Maybe not really the only the only thing you want to predict that it's not orbiting fitting, but\n",
    "\n",
    "00:52:36\n",
    "\n",
    "But it should just keep it. One example back propagation should just keep moving the model in the direction of having perfect accuracy and not one example.\n",
    "\n",
    "00:52:56\n",
    "\n",
    "You have enough nodes and parameters to represent the function\n",
    "\n",
    "00:54:07\n",
    "\n",
    "How's it going so far.\n",
    "\n",
    "00:54:17\n",
    "\n",
    "Everybody good\n",
    "\n",
    "00:54:43\n",
    "\n",
    "According to the bathroom THINK THEY'RE NOT TALKING WHAT'S GOING ON. I was off.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:55:10\n",
    "\n",
    "Todd, I'm probably still a little confused about why\n",
    "\n",
    "00:55:16\n",
    "\n",
    "The derivative of z one with respect to A one is, yeah, this point three times one point but yeah the whites, the point three, I guess, is\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:55:30\n",
    "\n",
    "Yeah, good question. So maybe because\n",
    "\n",
    "00:55:35\n",
    "\n",
    "It's a function of the input right and the input is going to be zero times point one.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "00:55:41\n",
    "\n",
    "Okay, yes.\n",
    "\n",
    "00:55:43\n",
    "\n",
    "Yeah.\n",
    "\n",
    " Kevin Stone\n",
    "\n",
    "00:55:49\n",
    "\n",
    "That note that should be Z one, not a one.\n",
    "\n",
    "00:55:53\n",
    "\n",
    "Year.\n",
    "\n",
    "00:55:56\n",
    "\n",
    "I think Z one is point\n",
    "\n",
    "00:55:59\n",
    "\n",
    "Five, seven or something.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:56:08\n",
    "\n",
    "Sir, what do you make them.\n",
    "\n",
    " Kevin Stone\n",
    "\n",
    "00:56:10\n",
    "\n",
    "Well, yeah, that little star to the right of the equation.\n",
    "\n",
    "00:56:16\n",
    "\n",
    "Oh yeah there. Oh.\n",
    "\n",
    "00:56:20\n",
    "\n",
    "I think XE one is point 570\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "00:56:23\n",
    "\n",
    "You're right, you're right.\n",
    "\n",
    "00:56:26\n",
    "\n",
    "That's why. Oh, that's what that was the confusion to\n",
    "\n",
    "00:56:34\n",
    "\n",
    "That's right. So it's been so much point five, seven times one minus point seven so be\n",
    "\n",
    "00:56:44\n",
    "\n",
    "Yeah look that up from what your previous\n",
    "\n",
    "00:56:50\n",
    "\n",
    "Thanks.\n",
    "\n",
    "00:57:33\n",
    "\n",
    "Was\n",
    "\n",
    "00:57:35\n",
    "\n",
    "A little bit.\n",
    "\n",
    "00:57:38\n",
    "\n",
    "It's more authentic.\n",
    "\n",
    "00:57:52\n",
    "\n",
    "It's tricky if you pick up a\n",
    "\n",
    "00:57:55\n",
    "\n",
    "Example looked a lot different textbooks and now they have a tried\n",
    "\n",
    "00:58:00\n",
    "\n",
    "And\n",
    "\n",
    "00:58:02\n",
    "\n",
    "It's tricky because there's a lot of different treatments of back propagation, with all the, you know, ultimately, it's all the\n",
    "\n",
    "00:58:08\n",
    "\n",
    "Same but\n",
    "\n",
    "00:58:11\n",
    "\n",
    "One of the ways that Scrabble back propagation works.\n",
    "\n",
    "00:58:14\n",
    "\n",
    "Like this is just kind of like trying to distill it down as much as possible. Just the pure calculations.\n",
    "\n",
    "00:58:52\n",
    "\n",
    "Well, somebody's got it.\n",
    "\n",
    "00:59:17\n",
    "\n",
    "Next time I to just be my iPod in real time.\n",
    "\n",
    "00:59:21\n",
    "\n",
    "And then it might be either\n",
    "\n",
    "00:59:24\n",
    "\n",
    "But these are all on\n",
    "\n",
    "00:59:48\n",
    "\n",
    "I'm sure\n",
    "\n",
    "01:00:07\n",
    "\n",
    "Know,\n",
    "\n",
    "01:00:17\n",
    "\n",
    "Getting the right direction or\n",
    "\n",
    "01:00:25\n",
    "\n",
    "You\n",
    "\n",
    " Craig Fleischman\n",
    "\n",
    "01:00:27\n",
    "\n",
    "Need to go back and redo the math.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:00:38\n",
    "\n",
    "You\n",
    "\n",
    " Bruno Todescan\n",
    "\n",
    "01:00:41\n",
    "\n",
    "Know, I, I got a little bit lost in the derivatives. I was trying to like compute the derivative on my own, like, make sure they knew where the derivative came from.\n",
    "\n",
    "01:00:52\n",
    "\n",
    "It's like apply the dirt. The derivatives to the cost function and then solve that. And I got a little bit my I really get the brush.\n",
    "\n",
    "01:01:02\n",
    "\n",
    "Yeah, kind of embarrassing but\n",
    "\n",
    "01:01:05\n",
    "\n",
    "I stopped there and then the number is kind of confused me so I'm I'm rambling into right now on the side me\n",
    "\n",
    " Amy Breden\n",
    "\n",
    "01:01:17\n",
    "\n",
    "By the way, the result I got lost with what we're supposed to do after we got the W one, W. W. W. Four. I didn't know what to do with them.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:01:27\n",
    "\n",
    "Would help you guys go back over real quick. What's more,\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "01:01:30\n",
    "\n",
    "Yes.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:01:41\n",
    "\n",
    "Okay, let's go, let's go to the whole thing real quick.\n",
    "\n",
    "01:01:45\n",
    "\n",
    "So again,\n",
    "\n",
    "01:01:48\n",
    "\n",
    "Stuff. We now can we add the logistic costs function sort of religiously activation function of all all three nodes in the hidden layer. The two hidden layer and then when I put could use a different one there. It'd be even easier if we use the regular really has the super simple derivative\n",
    "\n",
    "01:02:07\n",
    "\n",
    "Then we have a cost function together we're using a surrogate cost function that comes from the ACE think\n",
    "\n",
    "01:02:14\n",
    "\n",
    "We use a surrogate because continuously differentiable, and has a simple derivative and the derivative of it. The Lucy cost me to the surrogate, which is a cost function is of Y hat minus y.\n",
    "\n",
    "01:02:26\n",
    "\n",
    "Then just writing the chain rule when it is\n",
    "\n",
    "01:02:30\n",
    "\n",
    "You know is that this is only one decomposition composition. So whenever we're using the we're actually plant general twice.\n",
    "\n",
    "01:02:37\n",
    "\n",
    "And then the way to update rule, then we're just using a fixed clarity, right, a point to\n",
    "\n",
    "01:02:42\n",
    "\n",
    "That's the stuff, stuff we know\n",
    "\n",
    "01:02:45\n",
    "\n",
    "Here again, kind of the simplest network that's still interesting if we only permitted even small and only have one hit a note, it wouldn't be just enough and so began. You got one more down it's fully connected, you have a ton more calculations to make\n",
    "\n",
    "01:03:01\n",
    "\n",
    "And so\n",
    "\n",
    "01:03:04\n",
    "\n",
    "So break into a one and z one A one is before the activation function XE one is after the activation function on the output layer subcontinent Z three we're going to call it a while because the prediction.\n",
    "\n",
    "01:03:17\n",
    "\n",
    "Again, we're so reduce the costs are going to set at some point, we get to a particular example that example.\n",
    "\n",
    "01:03:26\n",
    "\n",
    "We get to a particular example in at that point the parameters have been set 2.1 point 2.3 point\n",
    "\n",
    "01:03:33\n",
    "\n",
    "Six. And then we have this as a particular example, and we want to compute the air and then update the parameters, based on the error and then see if we're more accurate on six which we should be\n",
    "\n",
    "01:03:44\n",
    "\n",
    "It should be if we're stochastic means that we're only doing one example at a time. The model should always get better in the direction of that example. Right. And so, so that's our example is x equals one, or sorry, x equals x, y equals zero, x\n",
    "\n",
    "01:04:00\n",
    "\n",
    "Two equals one and our prediction expected prediction is one\n",
    "\n",
    "01:04:06\n",
    "\n",
    "Video forward pass\n",
    "\n",
    "01:04:11\n",
    "\n",
    "Video forward pass somewhere here.\n",
    "\n",
    "01:04:22\n",
    "\n",
    "We are four paths.\n",
    "\n",
    "01:04:26\n",
    "\n",
    "And literally just means doing the three dot products right so we're gonna do adopt product times logistic function dot product trends logistic function dot product has logistic function that's a prediction. So if we were doing this.\n",
    "\n",
    "01:04:43\n",
    "\n",
    "Other code.\n",
    "\n",
    "01:04:46\n",
    "\n",
    "Produce that our code.\n",
    "\n",
    "01:04:50\n",
    "\n",
    "So if we go to reduce on our, on our GPU machine and vector eyes in it, we would just have three matrix operations and three multiplication.\n",
    "\n",
    "01:05:04\n",
    "\n",
    "So this would be super, super buzzed with hardware acceleration. Okay. And so then we want to do.\n",
    "\n",
    "01:05:10\n",
    "\n",
    "back propagation. We do in two steps. The first step is to back propagate the output layer to the hidden layer propagating we cycle back propagation or by copying the air right\n",
    "\n",
    "01:05:20\n",
    "\n",
    "And so, literally meaning that we're going to, we're trying to push the air back and it bypasses the network. And in doing so, partially attribute the air to the different weights.\n",
    "\n",
    "01:05:35\n",
    "\n",
    "And so we're going to do that by taking the air with respect to a particular ways\n",
    "\n",
    "01:05:42\n",
    "\n",
    "So the question here is basically question we're trying to answer it. This equation is what is the contribution of W five to the overall error. Right.\n",
    "\n",
    "01:05:50\n",
    "\n",
    "And we're getting that this by by using the general twice to decompose into three terms which we can just look up right, basically. And so the first one is, what does it. How does a three change with respect to Wi Fi, what changes by the one\n",
    "\n",
    "01:06:05\n",
    "\n",
    "Just the statement of all the neural network works.\n",
    "\n",
    "01:06:08\n",
    "\n",
    "The second thing is, how does the prediction itself change with respect to a change in a three fold the difference between the prediction and a three is the activation function right\n",
    "\n",
    "01:06:19\n",
    "\n",
    "And so it's going to change by the derivative of that commission function and then the change in the air with respect to the change of the prediction is what's called air, which is the cost function. So it's going to be derivative of that right\n",
    "\n",
    "01:06:33\n",
    "\n",
    "And so these are just basically statements of how we created our own network.\n",
    "\n",
    "01:06:40\n",
    "\n",
    "So if we multiply that all together, we should get a\n",
    "\n",
    "01:06:46\n",
    "\n",
    "change in w five\n",
    "\n",
    "01:06:51\n",
    "\n",
    "That we want to make after destiny by learning right\n",
    "\n",
    "01:06:57\n",
    "\n",
    "So, SO W five here. It was point five. Now we're going to dampen it with warranty rate of point to. And so as a result we get a new W five which is pointing right so we do the same thing for w six and then we want to do back propagation to the next layer.\n",
    "\n",
    "01:07:18\n",
    "\n",
    "For a hidden layer to our input layer assigning a partially attributing yard each of those w one w four so that we can update those always make sense.\n",
    "\n",
    "01:07:32\n",
    "\n",
    "It's the same process here. The only difference this time is that now to go from the air. The final error to z one\n",
    "\n",
    "01:07:45\n",
    "\n",
    "We have to incorporate everything we just previously computed\n",
    "\n",
    "01:07:50\n",
    "\n",
    "We added another layer, we would have to do the same thing but previously, but to go from like this would be the two and this one easy one. And we'd have to to change Eric's from one to two\n",
    "\n",
    "01:08:02\n",
    "\n",
    "Sorry, a change in the in the air with respect to z one in that case would be everything and two stops was going to keep composing\n",
    "\n",
    "01:08:11\n",
    "\n",
    "So this is going to turn that changes from the last layer in that it now contains the last layer is computations, which we could just look off from a lookup table we're implementing this right\n",
    "\n",
    "01:08:24\n",
    "\n",
    "So if we compete. This we should get that\n",
    "\n",
    "01:08:30\n",
    "\n",
    "W one does not change W two does not change W three increases by a little bit and W four increases by a little bit by little bit. I've been to 1000 right\n",
    "\n",
    "01:08:45\n",
    "\n",
    "So you also notice something else is that as a backdrop from left from right to left.\n",
    "\n",
    "01:08:51\n",
    "\n",
    "If we use an S shaped activation, we get smaller and smaller multiplication. So we tend to see less change in the earlier layers. Again, as part of the reason why people don't use these activation functions as much anymore to use Rails.\n",
    "\n",
    "01:09:05\n",
    "\n",
    "And similar activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript - Run forward propagation again using updated weights to get a new prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "01:09:12\n",
    "\n",
    "So now if you plug those in. If you plug in the same W on the same W 2.302 as W 3.402 is W 4.51 is w six five and point six one and w six and run that same train example again, you should get a prediction.\n",
    "\n",
    "01:09:35\n",
    "\n",
    "I believe it's\n",
    "\n",
    "01:09:38\n",
    "\n",
    "Is like slightly higher in the direction of expected\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "01:09:42\n",
    "\n",
    "That\n",
    "\n",
    " Amy Breden\n",
    "\n",
    "01:09:46\n",
    "\n",
    "I got point five minutes 6595\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:09:49\n",
    "\n",
    "Perfect. So we were successful at updating the ways in the direction of desired direction. Right.\n",
    "\n",
    " Rajiv Nair\n",
    "\n",
    "01:10:00\n",
    "\n",
    "Sorry. How did the initial, initial weights that he just randomly pick some are like how I know you probably did it. How is it typically done\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:10:08\n",
    "\n",
    "Yeah, so it's typically typically you pick random values on a very small. You don't want to pick zeros. Is there no I don't think zeros.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "01:10:18\n",
    "\n",
    "Yeah, doesn't update\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:10:21\n",
    "\n",
    "Why might you want random numbers. And so just to save a life. Everything\n",
    "\n",
    " Amy Breden\n",
    "\n",
    "01:10:28\n",
    "\n",
    "Optimize like a local.\n",
    "\n",
    "01:10:31\n",
    "\n",
    "Like minimum or maximum\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:10:36\n",
    "\n",
    "That could happen. Also, just if you have a uniform weights across the network.\n",
    "\n",
    "01:10:42\n",
    "\n",
    "Depending upon the distribution of values in your in your example you could end up not basically learning features learning interesting features. So you can just get some\n",
    "\n",
    "01:10:54\n",
    "\n",
    "If you want some variability in your in your\n",
    "\n",
    "01:10:57\n",
    "\n",
    "Network to start with.\n",
    "\n",
    " Rajiv Nair\n",
    "\n",
    "01:10:59\n",
    "\n",
    "It would be the end and weights between zero and one.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:11:03\n",
    "\n",
    "Now countries are typically people to very small numbers to start with. Okay. Yeah. Because if you if you pick two highways and the random\n",
    "\n",
    "01:11:13\n",
    "\n",
    "That can\n",
    "\n",
    "01:11:16\n",
    "\n",
    "Happen there.\n",
    "\n",
    "01:11:23\n",
    "\n",
    "If you pick your highways in a random. I think you have trouble with convergence.\n",
    "\n",
    "01:11:31\n",
    "\n",
    "Potentially\n",
    "\n",
    "01:11:36\n",
    "\n",
    "I'll see you don't want the fact that you don't want to like the random like you really want them to be zero. Like, you don't want the initial settings to influence with a network learns\n",
    "\n",
    "01:11:44\n",
    "\n",
    "But you can't sum zero because then you won't learn anything. Right. So you want them to be small around them.\n",
    "\n",
    " Padma Sridhar\n",
    "\n",
    "01:11:53\n",
    "\n",
    "And\n",
    "\n",
    "01:11:56\n",
    "\n",
    "Equally evenly balanced sort of network. If we have attributed the same way to every node, then when we do the SEC propagation, would we end up seeing an equal amount of air attributed to each other.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:12:11\n",
    "\n",
    "If the feature values were to be all the same across all features and then you're doing the first pass through. And so all your ways for the same thing. Yeah, equal attribution and you want to really learn anything useful. Yeah.\n",
    "\n",
    " Unknown Speaker\n",
    "\n",
    "01:12:25\n",
    "\n",
    "Okay.\n",
    "\n",
    " Shaji Kunjumohamed\n",
    "\n",
    "01:12:28\n",
    "\n",
    "Oh, for the bias terms basically for the bad back propagation\n",
    "\n",
    "01:12:34\n",
    "\n",
    "treated differently. The same. That's why.\n",
    "\n",
    " Todd Holloway\n",
    "\n",
    "01:12:36\n",
    "\n",
    "That's why they put bias in here just because they want they want added into the learning. It's the same. It's exactly the same process. Okay.\n",
    "\n",
    "01:12:44\n",
    "\n",
    "You can even think of when you when you go from like a like a logistic regression equation to a single hidden layer neural network.\n",
    "\n",
    "01:12:51\n",
    "\n",
    "You can think of the bias note as being the intercept with a, with the input always be in one. So you can think of, like, like if you do your homework is prediction and your equation is price equals\n",
    "\n",
    "01:13:03\n",
    "\n",
    "Alpha. Alpha is the intercept plus beta one times number of bass beta two times number of beds. You can think of it as being price equals one times alpha plus\n",
    "\n",
    "01:13:18\n",
    "\n",
    "Beta one of the replies, like there's a feature value there. They're just always one. And that's the wise.\n",
    "\n",
    "01:13:24\n",
    "\n",
    "So, so you can treat it. So really the bias is just another feature that is the same.\n",
    "\n",
    "01:13:30\n",
    "\n",
    "Same input.\n",
    "\n",
    "01:13:35\n",
    "\n",
    "Other questions.\n",
    "\n",
    "01:13:42\n",
    "\n",
    "Okay, I hope that was helpful, I hope that shed a little bit of light. If you have other questions. If you think about it more, let me know. Otherwise, I'll see you guys next time.\n",
    "\n",
    " Craig Fleischman\n",
    "\n",
    "01:13:53\n",
    "\n",
    "Thanks john\n",
    "\n",
    " Padma Sridhar\n",
    "\n",
    "01:13:54\n",
    "\n",
    "Thank you.\n",
    "\n",
    " Lee Moore\n",
    "\n",
    "01:13:55\n",
    "\n",
    "Thank you.\n",
    "\n",
    " Sonal Thakkar\n",
    "\n",
    "01:13:57\n",
    "\n",
    "Thank you.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
