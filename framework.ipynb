{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A framework for machine learning projects\n",
    "\n",
    "In general, supervised machine learning projects follow a certain flow. While the particulars of what happens in each step are different and may be more involved than others, usually you have the following steps. Steps 7 and 8 are kinda optional, but I would imagine tend to occur more often than not.\n",
    "\n",
    "## 1) Import libraries, set random seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(10) # any number is fine, pick your favorite\n",
    "# any others you might need..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Get your data and load it into your environment\n",
    "\n",
    "This can take a lot of forms depending on what kind of data you're working with. Common functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.read_csv()\n",
    "# using urllib.request() to call an API\n",
    "# json.dumps()\n",
    "# many others..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocess your data and do any feature engineering needed\n",
    "\n",
    "This is the part where you will likely spend most of your time. I always think of this as continuously asking the following questions:\n",
    "\n",
    "- What is the question I care about asking?\n",
    "- Does my data have the necessary fields/features in order to answer that question?\n",
    "- What transformations do I need to do to get the necessary fields?\n",
    "- Do I have any relevant time elements? How am I accounting for those from a feature perspective?\n",
    "- Are there features in my data that I can reasonably ignore/exclude from further analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common functions\n",
    "# don't really exist so much here... really dependent upon problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Split your data between training, validation, and possibly test sets\n",
    "\n",
    "For any project, you need both training and validation sets to have functional machine learning models. A test set allows you one extra step of being sure that your model generalizes well.\n",
    "\n",
    "Questions to ask and answer:\n",
    "\n",
    "- Do I have enough data that I can just subset it to create this split? Or do I need to get more creative and re-use my training data as validation (i.e., leave-one-out, cross-validation, etc)\n",
    "- Does my validation/test data reflect the new examples in the wild that this machine learning model would predict moving forward?\n",
    "- If there is significant class imbalance, do I have enough of each class in my validation set to assess the accuracy of my model on predicting each class?\n",
    "- If there are time elements, does my validation/test set take care to not \"leak\" any data about the future into making a prediction?\n",
    "\n",
    "In general, if you have lots of data, it is best practice to set aside both a validation set and a test set. You can assess model performance on your validation set, and then also look at the performance on the test set. You should expect to see a very strong correlation (coefficient ~1) between validation results and test results. If you choose to do this, DO NOT DIG FOR FURTHER INTERPRETATION IN YOUR TEST SET. It's a subtle thing, but it can be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common methods\n",
    "# random sampling\n",
    "# leave-one-out\n",
    "# cross-validation\n",
    "# windows / time slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Perform your machine learning!\n",
    "\n",
    "Now it's time to actually use that algorithm and get your predictions! This tends to be the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common framework\n",
    "# from sklearn.something import some_algorithm\n",
    "\n",
    "# if classification problem:\n",
    "# classifier = imported_classifier\n",
    "# trained_classifier = classifier.fit(X, y)\n",
    "# predictions = trained_classifier.predict(validation_set)\n",
    "\n",
    "# if regression, replace the word 'classifier' with 'regressor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Assess model performance and visualize\n",
    "\n",
    "From here, you want to see how well your model has done in predicting new data in your validation set. You also want to visualize, to see if anything funky is going on.\n",
    "\n",
    "Common metrics for classification:\n",
    "\n",
    "- Overall accuracy\n",
    "- Log loss / cross entropy\n",
    "- Confusion Matrix\n",
    "\n",
    "Common metrics for regression:\n",
    "\n",
    "- R^2\n",
    "- RMSE (root mean square error)\n",
    "- MAE (mean absolute error)\n",
    "\n",
    "It's important to note that you always want to have a baseline of a dumbest-possible model so that you can prove the fancy machine learning that you're doing is better than doing nothing. So for classification, you might think of predicting just one class for everything or rolling an n-sided dice to predict each class. Maybe for regression, you think about what your error would be if you just predicted the mean y value the whole time.\n",
    "\n",
    "Keep in mind that the sign of a good machine learning model is that it performs well on new data it hasn't seen (AKA generalizes well). So when making a judgement of whether a model is good or not, you really care about the validation metric, not the training metric.\n",
    "\n",
    "For visualizations, these will really vary on application. With very high dimensional problems, you may be well served showing actuals vs predictions. \n",
    "\n",
    "To assess overfitting, evaluate your metric on both your training and validation set. You will expect to see some sort of dropoff in performance in your validation set- however, the magnitude of this is a measure of how much you are overfitting. Again, even though overfitting is a bad thing, keep in mind that the thing we care about the most is performance on new samples, not necessarily the difference between training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Repeat steps 3-6 as needed\n",
    "\n",
    "You may have different assumptions about the data for different machine learning algorithms. Maybe one can take the data raw, the other one needs to have things transformed to normal distributions. Maybe your new algorithm can't take categorical variables, and you have to do some sort of processing here. Maybe you aren't even trying new algorithms, but you had an idea about how to make your existing one better by including more features.\n",
    "\n",
    "Whatever the case, start back at step 3 and do whatever you need to do in order to create new models and assess their performance relative to your baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# varies... may be a ton of code idk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Hyperparameter tuning\n",
    "\n",
    "Most machine learning algorithms have \"knobs\" that you can turn in order to see if you can get a better model. For example, with k-nearest-neighbors, you can can set K equal to whatever value you like to see if you get a better fit. Other models may have 5 - 20 different things you can manipulate. You should search the hyperparameters to see what gives you the best fit, if you have time. Performing visualizations may also help you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# varies depending on model used\n",
    "# typically use grid search or possibly bayesian tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Report your final results!\n",
    "\n",
    "It's important to make a judgement of which model you find to be the best. This may not always be the best performing one according to your metric, especially if you are worried about being able to interpret your model. If possible, you should try to look at how the model is making its predictions. What variables does it think are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
